---
layout: post
title: AUTO-RAG 김동규 원구원님
date: 2024-05-29 21:08 +0900
categories:
  - ETC
  - 세미나
tags: 
math: true
---


### 모든 문서에 완벽하게 적용되는 RAG는 없다

![](https://i.imgur.com/J7oVndX.png)


## 개선 방법

- RAG 성능 개선을 위한 방법론 너무 많음
- Langchain이나 LlamaIndex에 구현되어 있는것들은 빙산의 일각, 실제론 더 많은 모듈과 방법론 존재

![](https://i.imgur.com/wFBc2ru.png)

- 모든 데이터에 공통적으로 작동하는 방법은 없다 → 실험 필수 → 실험할 방법이 너무 많고 체계적이지 않다 → AUTO RAG 사용 권장

## AutoRAG 개념
- 많은 실험들을 효율적으로 자동화
- YAML파일 사용하여 RAG모듈 자동 테스트
- pdf등 raw 평가용 문서 생성 가능
- 자동으로 찾은 RAG 파이프라인을 api 서버, streamlit으로 사용 가능

![](https://i.imgur.com/U8P1qRY.png)


### RAG 평가 방법
- 질문 → 단락 → 생성(LLM)

단락:
- Retrieveal 평가
- Retrieve된 단락 A, B, C가 retrieval gt A와 일치하는가?

생성:
- 생성한 답변 평가
- 생성한 답변 A가 generation gt B(모법 답안)와 유사한가?

> gt를 어떻게 만드는가?
> - GPT, RAGAS 사용
## RAG 평가 데이터 만들기

### 좋은 평가 데이터 만들기
좋은 방법 순서대로:
1. 실제 유저 데이터 활용
2. 도메인 전문가와 함께 생성
3. human-in-the-loop 전략(LLM+사람)
4. llm만 사용

![](https://i.imgur.com/Q7HnnWy.png)

> 평가 데이터셋을 만들때엔 가장 똑똑한 LLM을 사용해야 한다

> 질문을 너무 많이 만들 필요 없다. 적지만 고품질 질문 100개 정도로 standard로 잡고 있다.


### RAG 평가 데이터셋 구조
- RAG 평가셋 준비
- Chuncking 완료된 문서에서 LLM 직접 사용 or 직접 예상 질문과 응답 생성
- 질문 생성에 사용된 chunk, 질문 그리고 모범 응답이 곧 QA데이터셋이 된다

![](https://i.imgur.com/SERZAIg.png)


- 두개의 Retreival gt를 모두 알아야만 답변할 수 있는 질문과 답변으로 생성

### Generation 지표 이해

![](https://i.imgur.com/VXV3fA3.png)

N-gram based metrics:
- 단어 비교방식
- 매우 싸고 빠름에도 성능이 나쁘지 않다

LM-based metrics
- LLM기반 대체

Sem Score
- 영어 데이터에서 사용성이 좋음

> 여유가 되면 LLM-based, 나머지로 보완하는 방식으로 사용
## AutoRAG 작동 원리

### 지원 RAG 모듈들

![](https://i.imgur.com/AEpFakO.png)

- 모든 경우의 수를 테스트 해보기 보단, 전 단계에서 가장 높은 결과만을 받아 평가 수행

### 단계별 평가 메트릭

![](https://i.imgur.com/j5pFCnK.png)


### 지표 이해하기

![](https://i.imgur.com/HwJZvjL.png)

- Recall: 정답 단락이 정해져있기 때문에 Retrieval Recall이 대부분 중요하다. 
- Precision: 환각 증세를 줄이는데 효과적 (retrival k수를 줄이는 방법): 모르는건 차라리 모르는게 낫다
- F1: Recall과 Precision 조화

> gt는 순서가 없어도 된다

> retrieval하는 단락의 수가 많을 수록 정답은 잘 가져오기 때문에 그 안에서 단락의 순위를 매기는것이 retrieval의 평가 하는데 많은 도움이 된다

> Precision을 높히는 노하우: Passage Filter
> - 특정 임계값 아래의 단락들 제거


## AutoRAG 사용해보기

### Config YAML file

![](https://i.imgur.com/yKJJyOi.png)

- node_line: 폴더 개념(추후에 기능 추가 예정)

![](https://i.imgur.com/2t6zmmU.png)

![](https://i.imgur.com/w8jKfSN.png)

- 여러 파라미터로 실험 가능

> LlamaIndex와 연동했기 때문에, 로컬 모델도 가능
> - VLLM 최적화 되어있음

![](https://i.imgur.com/uKvT10H.png)

> project_dir은 빈 디렉토리 사용하는것을 추천

## AutoRAG 결과값 해석


### 결과:

![](https://i.imgur.com/a3SqPLr.png)


### retrieval/summary.csv

![](https://i.imgur.com/4V86qLR.png)

![](https://i.imgur.com/iicGTUx.png)



### Dashboard

```bash
autorag dashboard --trial_dir ./your_project_dir/0
```

![](https://i.imgur.com/3bseblM.png)


![](https://i.imgur.com/jOQOPCc.png)

![](https://i.imgur.com/oCc6FEV.png)

### 최적화된 파이프라인 사용 Streamlit 실행
```bash
autorag run_web --trial_path ./sample_project/0
```

![](https://i.imgur.com/vwUChCF.png)



## Q&A

> 초보자들을 위한 base retriever 추천: 
> - BM25
> 	- 한국어를 사용할 경우 성능이 안좋음(형태소 분석기와 함께 사용하면 성능이 나음)
> 	- 키위와 같이 쓰면 OpenAI Embedding보다 좋을 수 있음

### Dataset 만드는 방법
- QA Dataset docs에 명시되어있는 format에 맞춰줘야함
	- `qa.parquet`file: `qid`, `query`, `retrieval_gt`, `generation_gt`
- metadata
	- `last_modeified_datetime` 필수(datetime.now()도 가능)



- 질문: RAG를 통한 결과에 이미지, 동영상을 결과를 얻으려면 어떻게 하는게 좋은지요? 
- 답변: 

- 질문: RAG 결과를 rdbms의 데이터의 기준으로 얻는 경우 어떻게 하는게 좀 더 정확한 결과를 얻을 수 있을지요
- 답변: text2sql 방법으로 query를 구성해서 가져오는 방법이 있다

- 질문: AutoRAG만으로 특정 도메인 사용자 대응이 충분한가요? 
- 답변: 파인튜닝을 시도해 보기 전에 RAG 테스트를 먼저 시도 추천(지식은 finetuning보다 RAG), LLM model 튜닝보다는 embedding model, 혹은 도메인 특화된 reranker모델에 초점을 맞추는게 나은 방향 같다

- 질문: RAG의 한계, FineTuning의 한계는 뭐라고 생각하시는지?
- 답변: 도메인 특화될수록 특정 용어를 모르는 경우가 많다(애매한 질문을 했을 경우), LLM기본 성능을 넘지 못한다


- 질문: 
-41:17