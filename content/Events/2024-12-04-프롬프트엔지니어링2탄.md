---
layout: post
title: "[테디노트]실무를 위한 프롬프팅 & 프롬프트 엔지니어링 2탄"
date: 2024-12-04 21:55 +0900
categories:
  - ETC
  - 세미나
tags: 
math: true
---

## LLM as a Judge

![](https://i.imgur.com/kgqdrXv.png)

AI의 Output을 LLM이 평가하는 방식

### Pairwise comparison
- A가 좋니 B가 좋니
### Single answer grading
- 인간의 만족도가 기준이 됨
### Reference-guided grading


![](https://i.imgur.com/3z3IFqf.png)

![](https://i.imgur.com/fNZKW06.png)

#### Chatbot Arena

![](https://i.imgur.com/stNHtlF.png)


![](https://i.imgur.com/St0JFR3.png)


>한국어로 물어보면 한국어를 잘하는 모델이 승리를 함 → 한국어 잘하는 모델 판단 가능





![](https://i.imgur.com/QFpDnuj.png)

## Discussion

![](https://i.imgur.com/0jf7BGD.png)

- LLM은 연속적인 범위 평가에 약한 모습을 보임 → 분류 평가 사용


## Research

### Methodology

![](https://i.imgur.com/W8Fg7U9.png)

- 정성적 평가와 정량적 평가를 합침

| 항목             | 일반 LLM 평가 방법                           | 현재 사용자 중심 연구                              |
|------------------|---------------------------------------------|--------------------------------------------------|
| **데이터 소스**  | 합성된 데이터                              | 실제 사용자 데이터                               |
| **데이터 셋**    | 단일 문장<br>연구자가 생성한 인공 문장 세트  | 단일 및 다중 턴 대화를 포함한 연속적 대화 내러티브 |
| **분석 초점**    | LLM 응답의 효율성 및 추론                   | 다양한 사용자 쿼리에 대한 LLM 응답의 적응성<br>사용자-AI 상호작용의 동적 특성<br>LLM에 대한 사용자 만족의 사례 및 조건 |
| **평가 방법**    | 자동화된 메트릭 (예: 정확도, ROUGE)<br>선호 기반 점수화 | 대화 분석을 위한 상호작용 언어학의 프레임워크<br>LLM과 인간 평가자의 응답 비교 분석 |

# sLM Prompt Enginnering: with Solar

## Literature Review

모델이 작을수록 편향이 심하다

![](https://i.imgur.com/yob9aoC.png)

페르소나, 롤 부여가 오히려 악영향을 끼쳤다

작은 모델일수록 reasoning steps에 민감하다



## Solar Cookbook


temperature 에 따른 범위

![](https://i.imgur.com/xx7wyB6.png)




### System Message vs User message

Use Cases:
- System Message: Long term 메모리
- User Message: 가변적인 메모리



CPU: Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz   3.00 GHz
RAM: 32GB
STORAGE: 1TB
NIC: Realtek 8822BE Wireless LAN 802.11ac PCI-E NIC
OS: Microsoft Windows Pro 10 64 bit
웹 브라우저: Chrome Version 131.0.6778,38 (Official Build) (64-bit)
기타 