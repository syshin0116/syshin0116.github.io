---
layout: post
title: 빅분기
date: 2023-09-22 07:41 +0900
categories:
  - 자격증
  - 빅분기
tags: 
math: true
---
## 회귀분석

### 회귀분석의 가정
1. 선형성: 독립변수(X)와 종속변수(Y)간의 선형성
2. 잔차의 3가지 가정(등분산성, 정규성, 독립성)
	1. 등분산성: 산점도
	2. 정규성:
		- H0(귀무가설: 정규분포를 따른다
		- H1(대립가설): not H0
		- 검정방법: Q-Q plot, 샤피오윌크 검정, 콜모고로프-스미스노프(K-S) 검정(비모수방법)
3. 독립성: 더빗왓슨 검정
	- 잔차: 표본의 실제값과 회귀분석 예측값과의 차이(실제값-예측값)


### 회귀분석에서 가설검정
(회귀모형, 회귀계수가 통계적으로 유의한지?)

1. 회귀모형: F검정, p-value < 0.05
	- H0(귀무): 회귀계수 = 0
	- H1(대립): 회귀계수 != 0
2. 회귀꼐소: t 검정, p-value < 0.0.5
	- H0(귀무): i번째 회귀계수 = 0
	- H1(대립): i번째 회귀계수 != 0


### 회귀식의 성능
1. 결정계수($R^2$)
	- 정의: 설명력, 전체 변동에서 회귀식이 설명가능한 변동의 비율
	- $R^2=SSR/SST = 1-(SSE/SST)$
		- SSR: 회귀식에 의해 설명되는 변동 ($\sum_{}^{}(\hat{y}-\bar{y})^2$) 예측값 - y평균값
		- SSE: 회귀식으로 설명 불가한 변동 ($\sum_{}^{}(y-\hat{y})^2$) 실제값 - 예측값
		- SST: 총 변동(SSR + SSE) ($\sum_{}^{}(y-\bar{y})^2$) 실제값 - y평균값
	- 범위: 0~1 사이 값을 가지며 클수록 성능이 좋음
	- 주의: 단 독립변수(X) 수가 증가 → $R^2$ 증가
		- $R^2$ adjust: 독립변수의 수가 증가하면 패널티를 줌

### 다중회귀 분석시 변수를 선택하는 방법
1. 전진 선택법(Forward selection): 하나씩 변수를 넣어보기
2. 후진 제거법(Backward elimination): 다 넣고 하나씩 빼기
3. 단계적 방법(Stepwise): 모든 조합 고려 가능
### 정규화 회귀(과적합을 피해보자)
1. 릿지 회귀(Ridge): 회귀계수가 0에 가깝게(변수 그대로), L2 규제
2. 라쏘 회귀(Lasso): 회귀계수가 0이 된다(변수 선택 효과), L1 규제
3. 엘라스틱넷(Elastic Net): 릿지, 라쏘 결합

### 영향점 진단
- 영향점은 보통 이상치로 회귀 직선에 크게 영향을 주는 데이터
#### 진단 방법
1. Leverage H
2. Cook's Distance
3. DFBETAS
4. DFFITS
#### 다중공선성
- 독립변수(X) 간에 상관관계가 존재하는 것
- VIF(분산팽창지수) 10 이상이면 다중공선성 존재
	- 다중공선성 있는 변수 제거해야함


## Classification(분류)
### 로지스틱 회귀 분석(지도학습 / 분류)
#### 설명:
- 종속변수(Y)가 범주형인 경우에 사용, 이진분류(0 아니면 1로 분류)
- 시그모이드 함수(S자형 곡선): Y가 1일 확률값을 구해준다(0~1 사이 값)
	- 임게값: 보통 p=0.5
	- 확률값이 0.5보다 크면 1로 분류, 그렇지 않으면 0으로 분류

### Support Vector Machine, SVM, 서포트 벡터 머신(지도학습, 회귀, 분류)
#### 설명:
- 데이터 세트를 분할하기 위한 최상의 **초평면**(Hyperplane)을 구함
- 특징: 
	- 비선형 데이터 분류시 **커널 함수**를 통해 다른 차원(차원증가)으로 맵핑하여 해결 성능이 좋으나 하이퍼파라미터(초매개변수) 영향을 많이 받는다.
	- 계산량이 많아서 시간 소요
- 하이퍼파라미터 C값 증가 = 하드마진(타이트하게)= 성능↑ = 과적합 위험


### 의사결정 나무, Decision Tree(지도학습/ 회귀, 분류)

#### 설명:
- 회귀일때(Y 연속성) 분리기준: 분산분석 F-통계량 p값, 분산의 감소량
- 분류일때(Y 범주형) 분리기준: 카이제곱 통계량 p값, 지니지수, 엔트로피 지수
- 대표적인 DT 알고리즘 CART특징: 지니지수(Y 범주), 분산의 감소량 사용(Y연속)

- 장점: 의사결정나무 시각화시 직관적 이해 쉬움, 비선형 분석 가능, 비모수적(가정 불필요)
- 단점: 이상치에 영향을 크게 받음, 성능이 그리 좋지 않음


### 앙상블 모형
- 여러가지 모형들을 만든 후에, 하나의 최종 결론을 내는 방법
	- 장점: 성능이 좋음, 일반화 성능이 좋음
#### 1. 배깅(bagging)
- **Bootstrap으로 데이터셋 생성 → 각 데이터셋마다 모델링 → 투표해서 최종 값 결정**
- Bootstrap: 단순 랜덤 복원 추출(샘플에 한번도 선태고디지 않는 원데이터가 발생할 수 있음, 약 36.8%)

#### 2. 부스팅(boosting)
- **예측력이 약한(weak) 모델에서 오류에 가중치를 줘서 더 좋은 모델로 발전시켜나감**
- 모델링 → 오분류데이터에 가중치 부여 → 모델링 → 오분류 데이터에 가중치 부여 순
- 종류: GBM, XGBoost, LightGBM(LGBM)

#### 3. 랜덤포레스트(Random Forest)
- **다수의 의사결정나무를 랜덤으로 만들어 그 결과값을 투표하여 최종 값 결정**
- 회귀: 평균, 분류: 투표
- 노이즈에 민감하지 않음
- 배깅보다 더 많은 무작위성(변수 임의선정하여 트리 생성)을 부여함

### K-NN(지도학습 / 회귀, 분류)
#### 설명:
- 장/단점: 원리가 간단하나, k값, 이상치에 따라 성능이 최지우지
- 키워드: 사례기반 학습(모델링 하지 안흥ㅁ), 게으른 모델


### Hard Voting / Soft Voting

![](https://i.imgur.com/DzfdDPC.png)
