---
layout: post
title: "[SeSAC]인공지능"
date: 2023-07-28 09:00:33 +0900
categories: [SeSAC, 온라인 강의]
tags: []
math: true
---

## 1. 데이터와 인공지능

### 데이터와 인공지능의 관계
- 인공지능을 이해하기 위해서는 먼저 빅데이터가 무엇인지 알아야 한다
- 빅테이터 알기 위해서는 데이터 테크놀러지(DT)가 무엇인지 알아야 한다

- 1990년대부터 인터넷 대중화
- 2000년대부터 인터넷에 일반 사람들이 글을 제3자에게 공유하는 정보 제공자 역할도 가능
- 수집할 수 잇는 데이터 양이 증가하면서 IT는 DT(Data Technology)로 확대

![](https://i.imgur.com/e9UTQVT.png)

### 빅테이터에 대한 오해
- 데이터의 양으로 구분하는것이 아니라, 전체 데이터 중 일부를 인포메이션으로 선별했는지 여부
- 발생한 데이터를 모두 수용했다면 빅 데이터이며, 그렇지 않다면 빅데이터라고 보기 힘들다

### DT의 중요성
- 사람의 판단으로 데이터를 처리하기에는 데이터의 양이 너무 많음
- 사람의 논리로 빅 데이터를 해석하면 일부의 데이터만 해석될 위험이 있음

![](https://i.imgur.com/sCWdnX8.png)

## 2. 머신러닝에 필요한 수학 개념

### 기초 선형대수
- 데이터를 다루는 법, 연립방정식을 사용하여 미지수의 값을 구하는 법 등에 사용되는 데이터분석의 기본적인 도구
	- 스칼라: 숫자 하나로 이루어진 데이터
	- 벡터 여러 숫자로 이루어진 데이터 레코드
	- 형렬: 벡터가 여러 개인 데이터 집합
	- 텐서: 같은 크기의 행렬이 여러개 있는 것


### 행렬의 덧셈과 뺄셈
- 같은 크기를 가진 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다

![](https://i.imgur.com/xTZff7D.png)

### 스칼라와 벡터/행렬의 곱셈
- 행렬에 스칼라를 곱하면 모든 원소에 스칼라를 곱하는 것과 같다

![](https://i.imgur.com/mImbmS4.png)


### 벡터와 벡터의 곱셈
- 내적
	1. 두 벡터의 차원(길이)이 같아야 한다
	2. 앞의 벡터가 행 벡터이고, 뒤의 벡터가 열 벡터이어야 한다

![](https://i.imgur.com/SmEGV0m.png)

### 일차함수

$y = ax + b(a!=0)$

- 기울기: a(y값의 증가량/x값의 증가량) =  $tan \theta$
- y 절편: x=0일 때 값
- x 절편: y=0일 때 값
- x절편이 a이고, y절편이 b인 직선의 방정식 $\frac{x}{a} + \frac{y}{b} = 1$
- 기울기가 m이고, 한 점 (a,b)를 지나는 직선 $y=m(x-a)+b$
- 기울기가 클수록 y축으로 경사가 급해지고, 작을수록 x축으로 경사가 급해짐

### 이차함수
$y = ax^{2} (a>0)$

- 위 그래프의 x를 p, y를 q만큼 평행이동:  $y = a(x-p)^{2} + q$
- x축으로 대칭: $y = -ax^{2}$
- 이차함수 x의 근은 y=0과 만나는 교점에 해당하므로 이차 방정식 =0의 해를 구하는 것과 같음

### 연립방정식

```python
import numpy as np
a = np.array([[5, -2], [4,5]])
b = np.array([-13,-6])

x = np.linalg.solve(a,b) 
print(x) # array([-2.33333333, 0.66666667])
```

### 미분
- 어떤 함수로부터 그 함수 기울기를 출력하는 새로운 함수를 만들어내는 작업
- 상수를 미분하면 0,
- 거듭제곱에 대한 값을 미분하면 $\frac{\mathrm{d}}{\mathrm{d} x}x^{n} = nx^{n-1}$

## 머신러닝에 필요한 확률과 통계

### 평균
```python
scores = np.array([42, 69, 56, 41, 57, 48, 65, 49, 65, 58])

np.mean(scores)
scores_df.mean()
```

### 중앙값
- 순서대로 정렬했을때 가장 가운데 값

```python
scores = np.array([42, 69, 56, 41, 57, 48, 65, 49, 65, 58])

np.median(scores)
scores_df.median()
```

### 최빈값

### 편차
- 평균으로부터 얼마나 떨어져있는가

```python
scores = np.array([42, 69, 56, 41, 57, 48, 65, 49, 65, 58])

mean = np.mean(scores)
deviation = scores - mean
```


### 분산
- 편차들을 제곱한 값의 합의 평균
- `np.var(scores)`

### 표준편차
- `np.sqrt(np.var(scores))`


### 데이터 정규화
- 시험점수가 동일하더라도, 쉬운 시럼과 어려운 시험에서의 60점은 상대적인 결과가 다르다
- 점수는 평균과 분삭에 따라 평가가 달라짐
- 데이터를 통일된 지표로 변환하는 것을 정규화라고 함

```python
z = (scores - np.mean(scores)) / np.std(scores)
```

- 표준화를 하면 평균이 0, 표준편차가 1이 된다

### 상관계수
- 상관계수는 -1~1사이의 값을 가짐
- 1 또는 -1에 가까워질수록 높은 상관관계
- 0에 가까울수록 낮은 상관관계
- `df.corr()`

### 모집단과 표본
- 모집단: 추측하고 싶은 관측 대상 전체
- 표본: 추측에 사용하는 관측 대상의 무작위 일부분

### 확률
- 확률변수: 취하는 값과 그 값이 나올 확률이 결정되어 있는 것

### 확률 분포
- 확률 변수가 어ㄸ허게 움직이는지는 나타낸 것

### 확률의 성질
- $f(x_{k}) \geq 0$
- $\sum_{k}f(x_{k})=1$ 

## 4. 지도학습에 필요한 데이터 가공하기

### 지도학습
- 문제와 답을 같이 학습함으로써 미지의 문제에 대한 올바른 답을 예측하고자 하는 방법

## 5. K근접 이웃 분류 모델 소개

- 가장 가까운 데이터를 탐색해서 값을 예측하는 모델

- 새로운 입력으로 들어온 데이터를 특정값으로 분류하는데, 현재 데이터와 가장 가까운 k개의 데이터를 찾아 가장 많은 분류 값으로 현재의 데이터를 분류하는 알고리즘

![](https://i.imgur.com/q4UUIGw.png)

### 장점
- 간단한 원리
- 우수한 성능
- 이진분류, 다중분류 모두 가능

### 단점
- 속도가 느림
- 하나의 예측을 분류하기 위해서 저장된 모든 데이터와 비교 해야함
- 적절한 K 값을 찾아야함

```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(문제집, 정답지)
knn.score(문제집, 정답지)
knn.predict(data)
```

## 6. 데이터를 분할해서 학습해야 하는 이유

### 훈련데이터와 시험데이터
- 훈련데이터로 학습하고, 훈련데이터로 평가한다면 정확도를 신뢰할 수 없음
- 훈련데이터와 시험데이터를 만들고, 훈련데이터로만 학습하고, 시험 데이터로 평가

### 데이터의 비율
- 최대한 훈련데이터에 많은 비율이 할당하는 것이 좋음
- 데이터가 적다면 최소 75:24 비율을 유지해주는 것이 좋음
- 데이터가 많다면 99:1 또는 99.9:1 수준으로 최대한 훈련데이터에 많은 비율을 할당하는 것이 좋음


```python
from sklearn.model_selection import train_test_split

X_train, Y_train, X_test, Y_test = train_test_split(train, test, test_siez=?)
```

## 7. 데이터 스케일링을 해야하는 이유

### 데이터 스케일링이란?
- 데이터 전처리 과정
- 데이터 특성의 범위 또는 분포를 같게 만드는 작업


### StandardScaler
```python
from sklearn.preprocessing import StandardScaler
```

- 모든 특성의 평균을 0으로, 표준편차를 1로 변환
- 이상치에 민감

### MinMaxScaler
```python
from sklearn.preprocessing import MinMaxScaler
```

- 특성 중 가장 작은 값을 0, 가장 큰 값을 1로 변환하여 0~1 사이의 값으로 만듬
- 이상치에 민감

### RobustScaler
```python
from sklearn.preprocessing import RobustScaler
```

- 중앙값과 사분위 값을 사용하여 **중앙값**을 뺴고, 사분위 값으로 나눔
- 이상치의 영향을 최소화 할 수 있음


## 8. K최근접 이웃 회귀 모델 소개

### K 최근접 이웃 회귀
- 새로운 값에 대해서 가장 가까운 데이터 k개의 평균 낸 값으로 새로운 값을 예측

![](https://i.imgur.com/ujjTFVM.png)
출처: https://coding-kindergarten.tistory.com/154

