---
layout: post
title: "[SeSAC]혼공 머신러닝+딥러닝 Ch7. 딥러닝"
date: 2023-08-30 14:23 +0900
categories:
  - SeSAC
  - 혼공 머신러닝+딥러닝
tags: []
math: true
---
## 07-1 인공 신경망

```python
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
```

### 패션 MNIST:
- MNIST(Modified National Institute of Standars and Technology Database)

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)  # (60000, 28, 28) (60000,)

print(test_input.shape, test_target.shape)  # (10000, 28, 28) (10000,)

import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()
```

![](https://i.imgur.com/5JVHqKJ.png)


```python
print([train_target[i] for i in range(10)])  # [9, 0, 0, 3, 0, 2, 7, 2, 5, 5]

import numpy as np

print(np.unique(train_target, return_counts=True))
# (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
```

### 로지스틱 회귀로 패션 아이템 분류하기

```python
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

print(train_scaled.shape)  # (60000, 784)

from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)

scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))  # 0.8196000000000001

```

### 인공신경망
#### 텐서플로와 케라스
```python
import tensorflow as tf

from tensorflow import keras
```

### 인공신경망으로 모델 만들기
```python
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)  # (48000, 784) (48000,)

print(val_scaled.shape, val_target.shape)  # (12000, 784) (12000,)

dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))

model = keras.Sequential(dense)

```

### 인공신경망으로 패션 아이템 분류하기

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

print(train_target[:10])  # [7 3 5 8 6 9 3 3 9 9]

model.fit(train_scaled, train_target, epochs=5)

model.evaluate(val_scaled, val_target)

# [0.45262545347213745, 0.8464999794960022]
```


---

### Perceptron

![](https://i.imgur.com/Do2k9E6.png)


### 인공지능 관련 인물들

#### **Marvin Minsky**:

- **정보**:
    - 1927년에 태어난 미국의 컴퓨터 과학자 및 인공지능 연구자.
    - MIT 미디어 랩의 공동 창립자로, 인공지능 및 인지과학 분야에서 큰 영향을 끼침.
    - "인공지능의 아버지" 중 한 명으로 불림.
    - 심리학 및 컴퓨터 과학을 접목시켜 인간의 지능을 모델링하려는 노력을 기울임.
    - "프레임(Frame)"이라는 개념을 소개하여 지식 표현을 발전시킴.
    - '퍼셉트론(Perceptron)'의 한계를 지적하며, 기호주의 접근법을 비판함.

#### **Frank Rosenblatt**:

- **정보**:
    - 1928년에 태어난 미국의 심리학자 및 컴퓨터 과학자.
    - "퍼셉트론(Perceptron)" 개념을 개발한 인공지능의 선구자 중 한 명.
    - 퍼셉트론은 초기 인공 신경망 모델로, 단순한 이진 분류를 수행하는 모델이다.
    - 기계 학습에서 신경망을 활용한 패턴 인식의 초기 기초를 마련한 인물로 평가됨.

#### **Geoffrey Everest Hinton**:

- **정보**:
    - 1947년에 태어난 캐나다의 컴퓨터 과학자.
    - '딥 러닝' 분야의 선구자 중 한 명으로 꼽힘.
    - 역전파 알고리즘을 개발하여 다층 퍼셉트론의 학습을 가능하게 함.
    - '드롭아웃(Dropout)'과 같은 기법을 도입하여 신경망의 과적합 문제를 해결하는데 기여.
    - 이미지 인식 등 다양한 분야에서 혁신적인 딥러닝 모델을 개발하며 인공지능 분야를 선도.

#### **Yann LeCun**:

- **정보**:
    - 1960년에 태어난 프랑스 출신의 컴퓨터 과학자 및 머신러닝 연구자.
    - "딥 러닝의 아버지"로 불리며, 현대 딥러닝의 발전에 큰 역할을 한 인물 중 하나.
    - 합성곱 신경망(CNN, Convolutional Neural Network)의 개념과 구조를 개발하여 컴퓨터 비전 분야에서의 중요한 기여를 함.
    - 손글씨 숫자를 인식하는 MNIST 데이터셋을 활용한 CNN의 성공적인 적용으로 유명.
    - 뉴욕대학교 (NYU) 교수이자, Facebook 인공지능 연구부 (FAIR)의 연구원이기도 함.

### 인공지능 파벌
#### **기호주의자(Symbolist or Symbolic AI)**:
    - 기호와 기호 간의 관계에 중점을 둠.
    - 추론과 논리를 핵심적인 요소로 간주.
    - 수작업으로 시스템에 규칙과 지식을 프로그래밍하여 지능적 행동을 유도.
    - 복잡한 지식을 처리하는 데 어려움이 있음.

#### **연결주의자(Connectionist or Connectionist AI)**:
    - 뉴런과 신경망을 모방한 인공 신경망 활용.
    - 데이터와 패턴을 학습하여 시스템이 스스로 지능적 행동을 개발하도록 함.
    - 신경망을 통한 특징 추출, 패턴 학습, 문제 해결에 초점.
    - 대량의 데이터와 학습이 필요하며 복잡한 문제에 강점을 보임.


**`sparse_categorical_crossentropy`**
- `categorical_crossentropy`: skit-learn의 `'log-loss'`와 
- `sparse`: 원핫-인코딩

batch_size: (dafault: 32)
- 의미: 32번의 평균을 구해서 가중치를 수정


## 07-2 신층 신경망

### 2개의 층

```python
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()

from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

from sklearn.model_selection import train_test_split

train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')
```

### 심층 신경망 만들기
```python
model = keras.Sequential([dense1, dense2])

model.summary()
```
![](https://i.imgur.com/5kBSNLh.png)

### 층을 추가하는 다른 방법
```python
model = keras.Sequential([
    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),
    keras.layers.Dense(10, activation='softmax', name='output')
], name='패션 MNIST 모델')

model.summary()
```
![](https://i.imgur.com/bM5EIp9.png)

```python
model = keras.Sequential()
model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```
![](https://i.imgur.com/FwO1QY1.png)

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/FWcTh7Q.png)

### 렐루 활성화 함수
```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```
![](https://i.imgur.com/eXea1OA.png)


```python
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/7mb59AY.png)

```python
model.evaluate(val_scaled, val_target)  # [0.3683287501335144, 0.8725833296775818]
```
![](https://i.imgur.com/9G5UDrm.png)

### 옵티마이저
```python
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')

sgd = keras.optimizers.SGD()
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')

sgd = keras.optimizers.SGD(learning_rate=0.1)

sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)

adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')

rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/VNHXvRc.png)

```python

model.evaluate(val_scaled, val_target)
```

![](https://i.imgur.com/2bWWD4m.png)


## 정리

Activation
- 히든 레이어의 activation은 성능에는 영향을 주지만 다 들어갈 수 있다
- 출력 레이어의 activation은 원하는 출력에 따라 정해야한다


딥러닝의 히든 레이어에는 비선형 activation을 써야한다: 선형을 쓸 순 있지만 의미가없음

시그모이드함수는 3번 이상 쓰이면 효과가 없어짐

Relu: 가장 흔히 쓰이는 hidden layer의 activation 함수

Ricky: Relu의 업그레이드


Flatten Layer: 2차원 데이터를 1차원 데이터로 바꿔준다(reshape으로 했었음)


옵티마이저: 
- Adam, RMSprop를 많이 씀

local minima, global minima

> auto-ml: 옵티마이징 과정, 하이퍼-파라미터 튜닝 등 과정을 자동화 하려는 학문


## 07-3 신경망 모델 훈련

# 신경망 모델 훈련

```python
# 실행마다 동일한 결과를 얻기 위해 케라스에 랜덤 시드를 사용하고 텐서플로 연산을 결정적으로 만듭니다.
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
```

## 손실 곡선


```python
from tensorflow import keras
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = \
    keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
    29515/29515 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
    26421880/26421880 [==============================] - 1s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
    5148/5148 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
    4422102/4422102 [==============================] - 1s 0us/step



```python
def model_fn(a_layer=None):
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28, 28)))
    model.add(keras.layers.Dense(100, activation='relu'))
    if a_layer:
        model.add(a_layer)
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model
```


```python
model = model_fn()

model.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     flatten (Flatten)           (None, 784)               0         
                                                                     
     dense (Dense)               (None, 100)               78500     
                                                                     
     dense_1 (Dense)             (None, 10)                1010      
                                                                     
    =================================================================
    Total params: 79,510
    Trainable params: 79,510
    Non-trainable params: 0
    _________________________________________________________________



```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=5, verbose=0)
```


```python
print(history.history.keys())
```

    dict_keys(['loss', 'accuracy'])



```python
import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```


![](https://i.imgur.com/YgN6osd.png)


```python
plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![](https://i.imgur.com/67VCWEJ.png)
```python
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0)
```


```python
plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```


    
![png](output_12_0.png)
    


## 검증 손실


```python
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target))
```


```python
print(history.history.keys())
```

    dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```


![](https://i.imgur.com/s5aIzjz.png)


```python
model = model_fn()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target))
```


```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```


![](https://i.imgur.com/SUYUgtT.png)




## 드롭아웃


```python
model = model_fn(keras.layers.Dropout(0.3))

model.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     flatten_4 (Flatten)         (None, 784)               0         
                                                                     
     dense_8 (Dense)             (None, 100)               78500     
                                                                     
     dropout (Dropout)           (None, 100)               0         
                                                                     
     dense_9 (Dense)             (None, 10)                1010      
                                                                     
    =================================================================
    Total params: 79,510
    Trainable params: 79,510
    Non-trainable params: 0
    _________________________________________________________________



```python
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target))
```


```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

![](https://i.imgur.com/Ns3tMKQ.png)


## 모델 저장과 복원


```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=10, verbose=0,
                    validation_data=(val_scaled, val_target))
```


```python
model.save_weights('model-weights.h5')
```


```python
model.save('model-whole.h5')
```


```python
!ls -al *.h5
```

    -rw-r--r-- 1 root root 333320 Jul 14 08:12 model-weights.h5
    -rw-r--r-- 1 root root 981176 Jul 14 08:12 model-whole.h5



```python
model = model_fn(keras.layers.Dropout(0.3))

model.load_weights('model-weights.h5')
```


```python
import numpy as np

val_labels = np.argmax(model.predict(val_scaled), axis=-1)
print(np.mean(val_labels == val_target))
```

    375/375 [==============================] - 1s 1ms/step
    0.8775

```python
model = keras.models.load_model('model-whole.h5')

model.evaluate(val_scaled, val_target)
```

    375/375 [==============================] - 1s 3ms/step - loss: 0.3388 - accuracy: 0.8775


    [0.3387581408023834, 0.8774999976158142]



## 콜백


```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5',
                                                save_best_only=True)

model.fit(train_scaled, train_target, epochs=20, verbose=0,
          validation_data=(val_scaled, val_target),
          callbacks=[checkpoint_cb])
```

    <keras.callbacks.History at 0x7883a41d8f40>

```python
model = keras.models.load_model('best-model.h5')

model.evaluate(val_scaled, val_target)
```

    375/375 [==============================] - 1s 3ms/step - loss: 0.3201 - accuracy: 0.8841

    [0.3201218545436859, 0.8840833306312561]


```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5',
                                                save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True)

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```


```python
print(early_stopping_cb.stopped_epoch)
```

    7



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```


![](https://i.imgur.com/LuGTCeF.png)



```python
model.evaluate(val_scaled, val_target)
```

    375/375 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8768

    [0.33070704340934753, 0.8768333196640015]



---
Layer:
- dropout
	- 출력 network중 일부를 날려버림
- dense
	- 
- flatten
	- reshape



pre-trained model: 대기업에서 멋지게 만들어놓고 공유하는 모델



콜백:
- 