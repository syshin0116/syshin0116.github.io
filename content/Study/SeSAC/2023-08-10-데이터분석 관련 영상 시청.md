---
layout: post
title: "[SeSAC]데이터분석 관련 영상 시청"
date: 2023-08-10 09:08 +0900
categories: [SeSAC, etc]
tags: []
---

## 프로젝트 발표 시청

### 땀내 줄이는 Data와 Feature 다루기


#### Intro: 땀내나는 이야기: 수학과 통계를 잘 모르는 개발자의 이야기

2017 Kaggle 데이터분석 프로젝트중 겪은 어려움 survey:
- 데이터 cleaning
- 결측치 처리

- Data: 우리에게 주어진 날것
- Feature: 신호와 소음

결측치 시각화 라이브러리: missingno

결측치 종류
1. MCAR(Missing Completely at Random)
	- 완전 무작위 결측
2. MAR(Missing at Random)
	- 임의적 결측 발생
3. NI(Non-ignorable)
	- 무시할 수 없는 결측 발생

결측치 처리 방법
1) 완전 제거법(List-wise deletion)
	- 가장 보편적으로 사용되는 방법
	- 결측율이 높을 대 정보 손실
	- 각 변수의 분산을 증가하게 하는 비효울적인 방법
2) 단일대체방법(Single Imputation)
	- 다양성을 반영하지 못함
	- 관측된 자료에 의존하는 문제
	- 구간을 나누어 구간별 평균/중앙값/최빈값을 구해서 대체
3) 다중대체방법(Multiple Inputation)
	- 결측이 완전 무작위로 발생한다는 가정
	- 대체가 가능한 값들의 분포로 부터 추출된 값으로 대체한 완전한 데이터세트를 만들어 대체


#### Imputation

scikit-learn Imputer
- 평균값, 중앙값, 최빈값으로 결측치를 채워줌

```python
from sklearn.preprocessing import Imputer
imputer = Imputer()
dataset['column'] = imputer.fit_transform(Dataset['column']).values.reshape(-1, 1))
```


predictive_imputer: R의 MissForest의 파이썬 버전


#### Feature Engineering

#### 범주형 데이터 다루기

One-Hot-(Vector|Encoding)

텍스트 데이터, 범수형 데이터 → 수치형 데이터
- 머신러닝이나 딥러닝 알고리즘은 수치로된 데이터만 이해
- 벡터에서 해당되는 하나의 데이터만 1로 변경해 주고 ㄴ나머지는 0으로 채워주는 것


scikit-learn LabelEncoder
- 범주형 데이터를 수치형 데이터로
```python
from sklearn.preprocessing import LabelEncoder

def lang_to_int(data)
	le = LabelEncoder()
	le.fit(['python', 'ruby', 'java'])
	data['lang'] = le.transform(data['lang'])
	return data
```

#### 수치형 데이터 다루기(연속형 vs 비연속형)

- 수치형 데이터 → 카테고리 데이터

```python
# 가족의 규모에 따라 카테고리로 구분
dataset.loc[dataset['FamilySize'] == 1, 'FsizeD'] = 'singleton'
dataset.loc[(dataset['FamilySize'] > 1) & (dataset['FamilySize'] < 5), 'FsizeD']
```

![](https://i.imgur.com/SVTVCMp.png)


#### 텍스트 데이터 다루기

BOW(bag of words)
- 단점: 순서를 완전히 무시함 → 문맥의 흐름을 이해하지 못함

N-gram
- 단어들을 묶어주기 때문에 문맥의 흐름을 잘 파악함
![](https://i.imgur.com/CwAq3A2.png)



TF-IDF( 가중치)
- Term frequency Inverse document fequency
- TF(단어 빈도, term frequency)
	- 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값
- DF(문서빈도, document frequency)
	- 값이 높을수록 문서에서 문요하다고 생각할 수 있지만, 단어 자체가 문서군 내에서 자주 사용되는 경우 단어가 흔하게 등장한다는 것을 의미
- TF-IDF는 TF와 IDF를 곱한 값


Error Data
- 잘못 입력된 정보
- 참고 할 수 있는 다른 Data가 있다면 보정
	- ex) 급여 정보가 없을 떄 보험료를 참고해서 급여를 예측

Outlier Data 다루기(이상치)
- Error Data와는 다름
- 데이터 혹은 샘플에서 동떨어진 값으로 모델에 편향을 줄 수 있음
- 정상치와 이상치의 기준을 어느 지점으로 할 것인지에 대한 모호함
- 도메인마다 이상치의 기준에 대한 정도가 다름 
	- ex) 의료 데이터는 이상치가 중요
	- ex) 국민 청원 데이터로 투표수를 예측한다고 할 때 특정 사회적 이슈에 따라 이상치 발생


![](https://i.imgur.com/QvBQed1.png)


Univariate transformation
- 일변량 비선형 변환
- 대부분의 모델은 각 특성이 정규분포와 비슷할 때 좋은 성능
- 오차가 정규분포를 따른 다는 가정하에 정규분포로 만들어 주는 과정이 필요함
-  로그함수를 복원할때는 지수함수를 사용
- 정수형 특성에 주로 사용
- 정수 Label Data에도 적용

MinMax Scaling
- 어느 한 변수가 분산이 너무 클때
- 평균을 0, 분산을 1로 만들어줌


#### 차원의 저주

차원 축소
- 데이터를 잘 설명할 수 있는 잠재공간 찾기
- 일정한 차원을 넘어가면 계속 성능이 떨어짐
- 작은 공간에 과적합 현상이 생겨 예측 성능이 떨어지는 것을 방지
- 데이터 압축(차원 축소), 시각화에 사용


PCA
- Feature Scaling

```python
import numpy as np
from sklearn.decomposition import PCA
X = np.array(\[\[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
pca = PCA(n_components=2)
pca.fit(X)

print(pca.explained_variance_ratio_)
print(pca.singular_values_)

# [0.99244289 0.00755711]
# [6.30061232 0.54980396]
```


t-SNE로 N 차원의 데이터를 2차원으로 변환
- N차원의 어레이를 시각화할수 없기 때문에 TSNE를 활용해 2차원 어레이로 만들면 시각화가 가능하다

#### Feature Pipeline
- 단어 

![](https://i.imgur.com/Z74MqlJ.png)

#### Multiprocessing
- `top-o cpu` 로 자원 사용 확인

자원 최대한 사용하도록 설정
- `xgb.XGBClassifier(nthread=-1)`
- `RandomForestRegressor(n_jobs=-1)`

