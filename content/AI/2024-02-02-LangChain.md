---
layout: post
title: LangChain
date: 2024-02-02 16:20 +0900
categories:
  - Deep-Learning
  - LangChain
tags: 
math: true
---

## Intro: 

ë­ì²´ì¸ ê³µë¶€í•˜ê³ ì ë­ì²´ì¸ docsë¥¼ ë³´ë‹¤ê°€ ìƒê°ë³´ë‹¤ ì–‘ì´ ë§ì•„ì„œ, ì²˜ìŒë¶€í„° ë›°ì–´ë“¤ê¸° ë³´ë‹¤ ì¼ë‹¨ ë”°ë¼í•´ë³¼ ìœ íŠœë¸Œë‚˜ ë¸”ë¡œê·¸ë¥¼ ê²½í—˜í•´ë³¸ í›„ì— official documentationì„ ì •ë…í•´ ë³´ë ¤ í•œë‹¤. ì…ë¬¸ìš©ìœ¼ë¡œ ê´œì°®ì•„ ë³´ì´ëŠ” ìœ íˆ½ì„ ì°¾ì•„ ë”°ë¼í•´ë³´ë ¤ í•œë‹¤. 

ìœ ë¶‘ link: ![https://www.youtube.com/watch?v=aWKrL4z5H6w](https://www.youtube.com/watch?v=aWKrL4z5H6w)



tags: Streamlit, OpenAI, Lamma2, Gemini Pro


Agenda: Practical Oriented
1. Environment Set up Open AI API Key
2. Building simpoe application with
	- LLM's -- LLM's Amd Chatmodels
	- PRomt Templates
	- Output Parser(PromptTemplate + LLM + OutputParser)


requirements.txtì— ipykernelì„ ì¶”ê°€í•˜ì§€ ì•ŠëŠ” ì´ìœ :
	ê°œë°œ ë‹¨ê³„ì—ì„œë§Œ í•„ìš”í•˜ì§€ deployì‹œì—” í•„ìš”í•˜ì§€ ì•ŠìŒ


### PDF Query with Langchain and Cassandra DB
### Apache Cassandra(Astra DB)
- open source No-SQL database 
- scalability and high availability

#### DATAX
- tool used to create Cassandra DB


#### Vector Search
- enhances machine learning models by allowing similarity comparisons of embeddings
- a capability of Astra DB



## Agenda(Generative AI)

1. About LLama 2
2. Research Paper of LLama 2
3. Apply and download the llama2 model
4. Huggingface
5. End to End Project(Blog Generation LLM App)


### LLama2


![](https://i.imgur.com/S4Ysuim.png)


#### research paper:

https://arxiv.org/pdf/2307.09288.pdf

- Training Details
	- Adopted most pretraining settings and model architecture from Lamma 1
	- Standard transformer architecture
	- pre-normalization using RMSNorm
	- SwiGLU activation function
	- Hyperparameters
		- AdamW optimizer
		- cosine learning rate schedule, warmup 2000 steps
		- decay final learning rate down to 10% of peark learning rate
		- weight decay of 0.1
		- gradient clipping of 1.0

- Comparison to Lamma1: 
	- increased context length
	- GQA: grouped-query attention

![](https://i.imgur.com/WmosUsb.png)


- Training Hardware & Carbon Footprint
	- Meta's Research Super Cluster & internal production clusters
		- **NVIDIA A100**s for both clusters

![](https://i.imgur.com/wquoJ3D.png)


- Fine-tuning
	- instruction tuning & RLHF(Reinforced Learning and Human Feedback)
	- Supervised Fine-Tuning(SFT)

![](https://i.imgur.com/uFChR4F.png)


- Reward Modeling



### Blog Generation


- langchain.llms.CTransformers doc
	- https://python.langchain.com/docs/integrations/providers/ctransformers

![](https://i.imgur.com/yIlHym2.png)

![](https://i.imgur.com/ogrxVs9.png)


```python

import streamlit as st

from langchain.prompts import PromptTemplate

from langchain_community.llms import CTransformers

  

## Function to get response from Llama 2 model

  

### Llama2

llm = CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin',

model_type='llama',

config={'max_new_tokens':256,

'temperature':0.01})

  

print("llm called")

print(llm.invoke("what is the capital of USA?"))

def getLlama2Response(llm, input_text, no_words, blog_style):

## Prompt Template

print(f'getLlama2Response called: {llm}, {input_text}, {no_words}, {blog_style}')

template="""

Write a blog for {blog_style} job profile for a topic {input_text} within {no_words} words.

"""

  

prompt = PromptTemplate(input_variables=["blog_style", "input_text", "no_words"],

template=template)

## Generate the response from LLama 2 model

response = llm.invoke(prompt.format(blog_style=blog_style, input_text=input_text, no_words=no_words))

print(response)

return response

  

st.set_page_config(

page_title="Generate Blogs",

page_icon='ğŸ¤–',

layout='centered',

initial_sidebar_state='collapsed'

)

  

st.header("Generate Blogs")

  

input_text= st.text_input("Enter the Blog Topic")

  

## create two columns for additional fields

  
  

col1, col2 = st.columns([5, 5])

  

with col1:

no_words = st.text_input('No. of words')

  

with col2:

blog_style = st.selectbox('Writing the blog for',

('Researchers', 'Data Scientist', 'Common People'), index=0)

  

submit = st.button("Generate")

  

## Final response

if submit:

st.write(getLlama2Response(llm, input_text, no_words, blog_style))
```



### LLM APPS

1. Load pdf
2. Convert to text chunks
3. Use OpenAI Embeddings to convert to vectors
4. Store vectors to search DB(Pincecone)
5. When User Queries inputs, apply similiary search to get result


![](https://i.imgur.com/V0cMx9u.png)


![](https://i.imgur.com/pSEh6nC.png)
#### Pineconeì´ë€?

- **ê´€ë¦¬í˜• ë²¡í„° ê²€ìƒ‰ ì„œë¹„ìŠ¤**: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ê³ ì°¨ì› ë²¡í„°ë¥¼ ì €ì¥, ë²¡í„° ê°„ì˜ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŒ
- **ê³ ì„±ëŠ¥ & ìœ ì‚¬ì„± ê²€ìƒ‰ ì§€ì›**: ìˆ˜ë°±ë§Œ ë²¡í„°ë¥¼ ëª‡ ë°€ë¦¬ì´ˆ ì•ˆì— ê²€ìƒ‰. ìœ í´ë¦¬ë“œ, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“± ë‹¤ì–‘í•œ ì¸¡ì • ê¸°ì¤€ ì œê³µ
- **í™•ì¥ì„±**: í´ë¼ìš°ë“œ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ìë™ ìŠ¤ì¼€ì¼ë§
- **ë³´ì•ˆ ê°•í™”**: ë°ì´í„° ë³´í˜¸ ë° í”„ë¼ì´ë²„ì‹œì— ì´ˆì 
- **ê°„í¸í•œ API**: ì‰½ê²Œ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì•±ì— í†µí•©í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ì ì¹œí™”ì  API

#### ì‚¬ìš© ì‚¬ë¡€

- **ìì—°ì–´ ì²˜ë¦¬**: ë¬¸ì„œ, ì§ˆë¬¸-ë‹µë³€, ê¸°ì‚¬ ë“±ì˜ ìœ ì‚¬ì„± ê²€ìƒ‰ì— í™œìš©
- **ì´ë¯¸ì§€ ì¸ì‹ ë° ê²€ìƒ‰**: ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰ ê°€ëŠ¥
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ì‚¬ìš©ì ì„ í˜¸ë„ë‚˜ í–‰ë™ì„ ë²¡í„°ë¡œ í‘œí˜„í•´ ê°œì¸í™”ëœ ì¶”ì²œ ìƒì„±


