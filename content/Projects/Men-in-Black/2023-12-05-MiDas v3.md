---
layout: post
title: MiDas v3
date: 2023-12-05 16:03 +0900
categories:
  - ETC
  - Paper-Summary
tags: 
math: true
---

## Intro: 

Men in Black(도로 교통 법규 위반 차량 자동 탐지 & 구분) 프로젝트 중 다양한 물체와 블랙박스 차량간의 거리를 구하기 위해 방법을 찾던 중 MiDas를 참고하고자 정리해보았다.

- paper: https://arxiv.org/abs/2307.14460
- github: https://github.com/isl-org/MiDaS


- youtube using MiDas: https://www.youtube.com/watch?v=MNzdybzH0kM&t=330s
- youtube: https://www.youtube.com/watch?v=c_WbKfyt8pY
	- https://github.com/nicknochnack/CodeTHat-MiDaS
## MiDaS v3.1 – A Model Zoo for Robust Monocular Relative Depth Estimation

날짜: 2023년 7월 26일

#### Abstract
 - 새로운 encoder backbone을 가진 모델을을 제시
 - 바닐라 ViT와 거의 유사했던 MiDas v3.0과는 달리 BEiT, Wsin, Wsin V2, Next-ViT, LeViT등의 모델이 추가되었다
 - 그 결과 최고 28% 성능 향상

#### Introduction
- Monocular depth estimation 해결로 generative AI, 3D reconstruction, autonomous driving등의 downstream tasks에 영향 끼침
- 데이터셋 혼합과 규모 및 이동 불변성 손실 구축이 MiDas의 일반화를 가능케 했다(MiDas의 시초)
- 여러 분야에서 활약중인 Transformer 기반 encoder을 사용하였고, 깊이있는 비교를 위해 경쟁력있는 covolutional encoder도 추가하였다

> downstream tasks: 더 복잡한 후속 응용 분야

**목적: 백본들을 MiDaS 아키텍처에 통합하는 것을 설명하고, 사용 가능한 다양한 v3.1 모델들에 대한 철저한 비교와 분석을 제공하며, 미래의 백본들과 함께 MiDaS를 어떻게 사용할 수 있는지에 대한 지침을 제공하는 것**



- conventional encoder-decoder 형태를 따른다
	- encoder: image-classification network
- 기존 v1.0, v2.0 모델은 ResNet 기반 multi-scale 구조
- v2.1엔 EfficientNet-Lite backbone 출시(모바일 친화적)
- v3.0부터 vision transformers모델에 대한 연구를 시작했고, v3.1에서 이를 이어나가고자 함


##### 3.1.1 Published Models

- 5개의 encoder type 선정





---

#### 클라우드 bash 파일 돌아가고 있는지 확인하는 코드

```bash
ps -ef|grep bash
```

```bash
htop
```

